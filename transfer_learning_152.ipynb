{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import metrics\n",
    "from torchvision import transforms, models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from normalization import normalization_parameter\n",
    "from mydataloader import data_loader\n",
    "from classplot import class_plot\n",
    "\n",
    "from pretrained_resnet152 import pretrained_resnet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 150\n",
    "batch_size = 16\n",
    "\n",
    "# 이미지 형태 변형\n",
    "train_transforms = transforms.Compose([transforms.Resize((im_size,im_size)),\n",
    "                                       transforms.ToTensor()])\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(root = '../dataset/emotion6', transform = train_transforms)\n",
    "train_loader =  DataLoader(train_data, batch_size = batch_size , shuffle = True)\n",
    "mean,std = normalization_parameter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 형태 변형\n",
    "train_transforms = transforms.Compose([transforms.Resize((im_size,im_size)),\n",
    "                                        transforms.RandomResizedCrop(size=315, scale=(0.95, 1.0)),\n",
    "                                        transforms.RandomRotation(degrees=10),\n",
    "                                        transforms.RandomHorizontalFlip(),\n",
    "                                        transforms.CenterCrop(size=299),  # Image net standards\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean,std)])\n",
    "# 테스트 이미지는 변환 x\n",
    "test_transforms = transforms.Compose([transforms.Resize((im_size,im_size)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean,std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평균과 표준편차를 역수로 바꿈\n",
    "inv_normalize =  transforms.Normalize(\n",
    "    mean=-1*np.divide(mean,std),\n",
    "    std=1/std\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 데이터들의 폴더를 지정해줌\n",
    "train_data = torchvision.datasets.ImageFolder(root = '../dataset/emotion6', transform = train_transforms)\n",
    "#test_data = torchvision.datasets.ImageFolder(root = '../dataset/emotion6/test', transform = test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = data_loader(train_data, batch_size = batch_size)\n",
    "classes = train_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = {}\n",
    "for i in range(len(classes)):\n",
    "    decoder[classes[i]] = i\n",
    "encoder = {}\n",
    "for i in range(len(classes)):\n",
    "    encoder[i] = classes[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_plot(train_data,encoder,inv_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pretrained_resnet152()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크로스 엔트로피 loss\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 얼리스탑핑\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        #model save\n",
    "        #torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "# fold count 추가\n",
    "def train(model,dataloaders,criterion,num_epochs=10,lr=0.00001,batch_size=8,patience = None):\n",
    "    \n",
    "    since = time.time()\n",
    "    model.to(device)\n",
    "    best_acc = 0.0\n",
    "    phase1 = dataloaders.keys()\n",
    "    losses = list()\n",
    "    acc = list()\n",
    "    \n",
    "    if(patience!=None):\n",
    "        earlystop = EarlyStopping(patience = patience,verbose = True)\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch:',epoch)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        lr = lr*0.8\n",
    "        if(epoch%10==0):\n",
    "            lr = 0.0001\n",
    "\n",
    "        for phase in phase1:\n",
    "            if phase == ' train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            total = 0\n",
    "            j = 0\n",
    "            for  batch_idx, (data, target) in enumerate(dataloaders[phase]):\n",
    "                data, target = Variable(data), Variable(target)\n",
    "                data = data.type(torch.cuda.FloatTensor)\n",
    "                target = target.type(torch.cuda.LongTensor)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                _, preds = torch.max(output, 1)\n",
    "                running_corrects = running_corrects + torch.sum(preds == target.data)\n",
    "                running_loss += loss.item() * data.size(0)\n",
    "                j = j+1\n",
    "                if(phase =='train'):\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                if batch_idx % 300 == 0:\n",
    "                    print('{} Epoch: {}  [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tAcc: {:.6f}'.format(phase,epoch, batch_idx * len(data), len(dataloaders[phase].dataset),100. * batch_idx / len(dataloaders[phase])\n",
    "                                                                                                 , running_loss/(j*batch_size),running_corrects.double()/(j*batch_size)))\n",
    "            epoch_acc = running_corrects.double()/(len(dataloaders[phase])*batch_size)\n",
    "            epoch_loss = running_loss/(len(dataloaders[phase])*batch_size)\n",
    "            if(phase == 'val'):\n",
    "                earlystop(epoch_loss,model)\n",
    "\n",
    "            if(phase == 'train'):\n",
    "                losses.append(epoch_loss)\n",
    "                acc.append(epoch_acc)\n",
    "            print(earlystop.early_stop)\n",
    "        if(earlystop.early_stop):\n",
    "            print(\"Early stopping\")\n",
    "            #model save\n",
    "            #model.load_state_dict(torch.load('./checkpoint.pt'))\n",
    "            break\n",
    "        print('{} Accuracy: '.format(phase),epoch_acc.item())\n",
    "    return losses,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader):\n",
    "    running_corrects = 0\n",
    "    running_loss=0\n",
    "    pred = []\n",
    "    true = []\n",
    "    pred_wrong = []\n",
    "    true_wrong = []\n",
    "    image = []\n",
    "    sm = nn.Softmax(dim = 1)\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        data = data.type(torch.cuda.FloatTensor)\n",
    "        target = target.type(torch.cuda.LongTensor)\n",
    "        classifier.eval()\n",
    "        output = classifier(data)\n",
    "        loss = criterion(output, target)\n",
    "        output = sm(output)\n",
    "        _, preds = torch.max(output, 1)\n",
    "        running_corrects = running_corrects + torch.sum(preds == target.data)\n",
    "        running_loss += loss.item() * data.size(0)\n",
    "        preds = preds.cpu().numpy()\n",
    "        target = target.cpu().numpy()\n",
    "        preds = np.reshape(preds,(len(preds),1))\n",
    "        target = np.reshape(target,(len(preds),1))\n",
    "        data = data.cpu().numpy()\n",
    "        \n",
    "        for i in range(len(preds)):\n",
    "            pred.append(preds[i])\n",
    "            true.append(target[i])\n",
    "            if(preds[i]!=target[i]):\n",
    "                pred_wrong.append(preds[i])\n",
    "                true_wrong.append(target[i])\n",
    "                image.append(data[i])\n",
    "      \n",
    "    epoch_acc = running_corrects.double()/(len(dataloader)*batch_size)\n",
    "    epoch_loss = running_loss/(len(dataloader)*batch_size)\n",
    "    print(epoch_acc,epoch_loss)\n",
    "    return true,pred,image,true_wrong,pred_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_plot(loss):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(loss)\n",
    "    plt.title(\"Training loss plot\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "    \n",
    "def acc_plot(acc):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(acc)\n",
    "    plt.title(\"Training accuracy plot\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.show()\n",
    "    \n",
    "# To plot the wrong predictions given by model\n",
    "def wrong_plot(n_figures,true,ima,pred,encoder,inv_normalize):\n",
    "    print('Classes in order Actual and Predicted')\n",
    "    n_row = int(n_figures/3)\n",
    "    fig,axes = plt.subplots(figsize=(14, 10), nrows = n_row, ncols=3)\n",
    "    for ax in axes.flatten():\n",
    "        a = random.randint(0,len(true)-1)\n",
    "    \n",
    "        image,correct,wrong = ima[a],true[a],pred[a]\n",
    "        image = torch.from_numpy(image)\n",
    "        correct = int(correct)\n",
    "        c = encoder[correct]\n",
    "        wrong = int(wrong)\n",
    "        w = encoder[wrong]\n",
    "        f = 'A:'+c + ',' +'P:'+w\n",
    "        if inv_normalize !=None:\n",
    "            image = inv_normalize(image)\n",
    "        image = image.numpy().transpose(1,2,0)\n",
    "        im = ax.imshow(image)\n",
    "        ax.set_title(f)\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "def performance_matrix(true,pred):\n",
    "    precision = metrics.precision_score(true,pred,average='macro')\n",
    "    recall = metrics.recall_score(true,pred,average='macro')\n",
    "    accuracy = metrics.accuracy_score(true,pred)\n",
    "    f1_score = metrics.f1_score(true,pred,average='macro')\n",
    "    print('Precision: {} Recall: {}, Accuracy: {}: ,f1_score: {}'.format(precision*100,recall*100,accuracy*100,f1_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,dataloaders,criterion,num_epochs=10,lr=0.0001,batch_size=8,patience = None,classes = None):\n",
    "    dataloader_train = {}\n",
    "    losses = list()\n",
    "    accuracy = list()\n",
    "    key = dataloaders.keys()\n",
    "    \n",
    "    for phase in key:\n",
    "        if(phase == 'test'):\n",
    "            perform_test = True\n",
    "        else:\n",
    "            dataloader_train.update([(phase,dataloaders[phase])])\n",
    "            \n",
    "    losses,accuracy = train(model,dataloader_train,criterion,num_epochs,lr,batch_size,patience)\n",
    "    \n",
    "    error_plot(losses)\n",
    "    acc_plot(accuracy)\n",
    "    \n",
    "    if(perform_test == True):\n",
    "        true,pred,image,true_wrong,pred_wrong = test(dataloaders['test'])\n",
    "        wrong_plot(12,true_wrong,image,pred_wrong,encoder,inv_normalize)\n",
    "        performance_matrix(true,pred)\n",
    "        \n",
    "        if(classes !=None):\n",
    "            plot_confusion_matrix(true, pred, classes= classes,title='Confusion matrix, without normalization')\n",
    "            \n",
    "    return accuracy, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "loss = []\n",
    "for data in dataloaders:\n",
    "    acc, losses = train_model(classifier,data,criterion,50, patience = 10 , batch_size = batch_size , classes = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy : \", acc)\n",
    "print(\"loss : \", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
